{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa2fde3",
   "metadata": {},
   "source": [
    "## 2.5 Problem: BPE Training on TinyStories (2 points)\n",
    "`(train_bpe_tinystories)`\n",
    "\n",
    "**(a)** Train a byte-level BPE tokenizer on the **TinyStories** dataset, using a maximum vocabulary size of **10,000**. Make sure to add the TinyStories <|endoftext|> special token to the vocabulary. Serialize the resulting vocabulary and merges to disk for further inspection. How many hours and memory did training take? What is the longest token in the vocabulary? Does it make sense?\n",
    "\n",
    "**Resource requirements:** ≤ 30 minutes (no GPUs), ≤ 30GB RAM \n",
    "\n",
    "**Hint** You should be able to get under 2 minutes for BPE training using multiprocessing during pretokenization and the following two facts:\n",
    "\n",
    "- The <|endoftext|> token delimits documents in the data files.\n",
    "\n",
    "- The <|endoftext|> token is handled as a special case before the BPE merges are applied.\n",
    "\n",
    "**(b)** Profile your code. What part of the tokenizer training process takes the most time?\n",
    "\n",
    "### **Answers**\n",
    "\n",
    "**(a) Time & Memory Usage**\n",
    "\n",
    "TinyStoriesV2-GPT4-train.txt, vocab_size = 10000.\n",
    "\n",
    "Without inverted index optimization:\n",
    "- num_processes = 4: \n",
    "    - time: 172.03s\n",
    "- num_processes = 40:\n",
    "    - time: 60.81s\n",
    "    - memory: peak Python heap: 67.1 MB, peak RSS: 144.1 MB\n",
    "\n",
    "After optimizing merge pair with inverted index + heap with lazy deletion:\n",
    "- num_processes = 40:\n",
    "    - time: **16.6s**\n",
    "    - memory: peak Python heap: 126.2 MB, peak RSS: 391.0 MB\n",
    "\n",
    "The longest token: `id=7160, len=15 bytes, text=' accomplishment'`\n",
    "\n",
    "**(b) Code Profiling** \n",
    "\n",
    "Notes: Machine info\n",
    "- OS: Ubuntu 22.04.5 LTS (kernel 6.8 on x86_64)\n",
    "- CPU: AMD EPYC 9V84 (40 vCPUs visible)\n",
    "- RAM: 314 GiB total (≈296 GiB available right now)\n",
    "- Disk: Root filesystem ≈2.0 TB total, ≈2.0 TB free\n",
    "- GPU: NVIDIA GA103 detected on PCIe (PCI ID 10de:2321), but NVIDIA driver/module is not loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5039339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "\n",
    "def save_tokenizer_yaml(vocab, merges, fname):\n",
    "    \"Save vocab and merges to a YAML file with UTF-8 decoding for readability.\"\n",
    "    # Convert bytes → string for readability\n",
    "    vocab_serializable = {\n",
    "        k: v.decode(\"utf-8\", errors=\"replace\") if isinstance(v, bytes) else v\n",
    "        for k, v in vocab.items()\n",
    "    }\n",
    "    merges_serializable = [\n",
    "        (a.decode(\"utf-8\", errors=\"replace\"), b.decode(\"utf-8\", errors=\"replace\"))\n",
    "        for a, b in merges\n",
    "    ]\n",
    "\n",
    "    # Ensure the parent directory exists before writing\n",
    "    dirpath = os.path.dirname(fname)\n",
    "    if dirpath:\n",
    "        os.makedirs(dirpath, exist_ok=True)\n",
    "\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.dump(\n",
    "            {\"vocab\": vocab_serializable, \"merges\": merges_serializable},\n",
    "            f,\n",
    "            allow_unicode=True,\n",
    "            sort_keys=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7811ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "from cs336_basics.bpe import run_train_bpe\n",
    "\n",
    "file_name = \"data/TinyStoriesV2-GPT4-train.txt\"\n",
    "vocab_size = 10000\n",
    "num_processes = os.cpu_count()\n",
    "\n",
    "### Training\n",
    "\n",
    "start = time.perf_counter()\n",
    "vocab, merges = run_train_bpe(file_name, vocab_size, [\"<|endoftext|>\"], num_processes)\n",
    "elapsed_s = time.perf_counter() - start\n",
    "print(f\"time: {elapsed_s:.2f}s\")\n",
    "\n",
    "### Longest token in the vocabulary\n",
    "\n",
    "save_tokenizer_yaml(vocab, merges, \"artifacts/tinystories_bpe.yaml\")\n",
    "longest_id, longest_bytes = max(vocab.items(), key=lambda kv: len(kv[1]))\n",
    "print(f\"longest token: id={longest_id}, len={len(longest_bytes)} bytes, text={longest_bytes.decode('utf-8','replace')!r}\")\n",
    "# longest token: id=7160, len=15 bytes, text=' accomplishment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Time & Memory usage; Profile code\n",
    "\n",
    "import tracemalloc, resource, time\n",
    "import os\n",
    "import cProfile, pstats, io\n",
    "from cs336_basics.bpe import run_train_bpe\n",
    "\n",
    "file_name = \"data/TinyStoriesV2-GPT4-train.txt\"\n",
    "vocab_size = 10000\n",
    "\n",
    "# Start fresh\n",
    "pr = cProfile.Profile()\n",
    "pr.enable()\n",
    "tracemalloc.start()\n",
    "start = time.perf_counter()\n",
    "\n",
    "# Training\n",
    "vocab, merges = run_train_bpe(file_name, vocab_size, [\"<|endoftext|>\"], num_processes=os.cpu_count())\n",
    "\n",
    "# Stop and report\n",
    "elapsed_s = time.perf_counter() - start\n",
    "pr.disable(); \n",
    "cur, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "peak_rss_mb = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024\n",
    "print(f\"time: {elapsed_s:.2f}s ({elapsed_s/3600:.2f} h), peak Python heap: {peak/1e6:.1f} MB, peak RSS: {peak_rss_mb:.1f} MB\")\n",
    "\n",
    "s=io.StringIO(); \n",
    "pstats.Stats(pr, stream=s).sort_stats('cumtime').print_stats(30)\n",
    "print(s.getvalue())\n",
    "\n",
    "\"\"\" Profiling Results:\n",
    "\n",
    "time: 238.98s (0.07 h), peak Python heap: 126.2 MB, peak RSS: 391.0 MB\n",
    "         19354352 function calls (19353456 primitive calls) in 238.873 seconds\n",
    "\n",
    "   Ordered by: cumulative time\n",
    "   List reduced from 408 to 30 due to restriction <30>\n",
    "\n",
    "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
    "      205    0.054    0.000  369.225    1.801 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/asyncio/base_events.py:1970(_run_once)\n",
    "        5    0.000    0.000  238.977   47.795 /home/haoru/.cache/uv/archive-v0/ZzdUiPCnVvyZrxKy_ds_G/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3663(run_code)\n",
    "     10/5    0.000    0.000  238.977   47.795 {built-in method builtins.exec}\n",
    "      205    0.090    0.000  214.202    1.045 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/selectors.py:435(select)\n",
    "       40    0.056    0.001  154.003    3.850 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/multiprocessing/queues.py:96(get)\n",
    "       40    0.000    0.000  153.372    3.834 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:208(recv_bytes)\n",
    "      205  141.259    0.689  142.454    0.695 {method 'poll' of 'select.epoll' objects}\n",
    "     9743   45.666    0.005   75.383    0.008 /home/haoru/cs336/cs336-hw1/cs336_basics/bpe.py:159(merge_pair_and_update_counts)\n",
    "       40    0.000    0.000   13.920    0.348 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:429(_recv_bytes)\n",
    "       80    0.001    0.000   13.919    0.174 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:390(_recv)\n",
    "      280   13.914    0.050   13.914    0.050 {built-in method posix.read}\n",
    "  7123718   10.989    0.000   10.989    0.000 {method 'get' of 'dict' objects}\n",
    "        1    0.601    0.601    9.150    9.150 /home/haoru/cs336/cs336-hw1/cs336_basics/bpe.py:322(run_train_bpe)\n",
    "        1    0.092    0.092    8.549    8.549 /home/haoru/cs336/cs336-hw1/cs336_basics/bpe.py:261(train_bpe_on_pretokens)\n",
    "  1456506    4.021    0.000    4.021    0.000 {method 'add' of 'set' objects}\n",
    "     9743    1.156    0.000    3.519    0.000 /home/haoru/cs336/cs336-hw1/cs336_basics/bpe.py:109(find_most_frequent_pair)\n",
    "  1081253    3.467    0.000    3.467    0.000 {method 'setdefault' of 'dict' objects}\n",
    "  1079593    3.291    0.000    3.291    0.000 {method 'discard' of 'set' objects}\n",
    "  1368847    3.033    0.000    3.033    0.000 {method 'append' of 'list' objects}\n",
    "  2632440    2.583    0.000    2.583    0.000 {built-in method builtins.len}\n",
    "   749810    0.626    0.000    2.047    0.000 /home/haoru/cs336/cs336-hw1/cs336_basics/bpe.py:65(_desc_key)\n",
    "   551932    1.951    0.000    1.951    0.000 {built-in method _heapq.heappop}\n",
    "  1499620    0.766    0.000    1.421    0.000 /home/haoru/cs336/cs336-hw1/cs336_basics/bpe.py:47(_inv)\n",
    "        1    0.730    0.730    1.189    1.189 /home/haoru/cs336/cs336-hw1/cs336_basics/bpe.py:72(build_pair_counts_and_index)\n",
    "   747725    1.189    0.000    1.189    0.000 {built-in method _heapq.heappush}\n",
    "   606448    0.993    0.000    0.993    0.000 {method 'pop' of 'dict' objects}\n",
    "   277855    0.945    0.000    0.945    0.000 {method 'items' of 'dict' objects}\n",
    "       41    0.504    0.012    0.702    0.017 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/collections/__init__.py:673(update)\n",
    "       40    0.572    0.014    0.572    0.014 {built-in method _pickle.loads}\n",
    "       40    0.002    0.000    0.203    0.005 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/multiprocessing/process.py:110(start)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcface22",
   "metadata": {},
   "outputs": [],
   "source": [
    "### BPE Training on TinyStories w/O Inverted Index Optimization\n",
    "\n",
    "# Profiling result below shows:\n",
    "#    run_train_bpe: 125.404s total runtime (baseline to compare others).\n",
    "#    train_bpe_on_pretokens: 94.138s (~75% of total) on BPE training i.e. merge loop.\n",
    "# \n",
    "# Within the merge loop,\n",
    "#    the dominant cost is repeatedly find_most_frequent_pair (65.180s; max over a large dict pair_counts).\n",
    "#         builtins.max inside it: tottime 44.085 s\n",
    "#         the key lambda used by max: 369,218,707 calls, 21.031 s\n",
    "# \n",
    "# ~30.62s (~24%) is spent receiving pretoken counts from worker processes via multiprocessing Queue.\n",
    "#\n",
    "# How to read cProfile table:\n",
    "#    Each row is a function.\n",
    "#\n",
    "#    ncalls: how many times it was called\n",
    "#    tottime: time spent in that function body only (exclusive)\n",
    "#    cumtime: time spent in that function plus all functions it called (inclusive)\n",
    "#    percall: divides by ncalls\n",
    "#    Sorted by cumtime (largest “overall cost” at top)\n",
    "\n",
    "\"\"\" Profiling Results:\n",
    "\n",
    "# Results\n",
    "   Ordered by: cumulative time\n",
    "   List reduced from 126 to 30 due to restriction <30>\n",
    "\n",
    "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
    "        1    0.041    0.041  125.404  125.404 /home/haoru/cs336/cs336-hw1/cs336_basics/train_bpe.py:244(run_train_bpe)\n",
    "        1    4.192    4.192   94.138   94.138 /home/haoru/cs336/cs336-hw1/cs336_basics/train_bpe.py:184(train_bpe_on_pretokens)\n",
    "     9743    0.038    0.000   65.180    0.007 /home/haoru/cs336/cs336-hw1/cs336_basics/train_bpe.py:71(find_most_frequent_pair)\n",
    "     9743   44.085    0.005   65.116    0.007 {built-in method builtins.max}\n",
    "       40    0.062    0.002   30.797    0.770 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/multiprocessing/queues.py:96(get)\n",
    "       40    0.000    0.000   30.621    0.766 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:208(recv_bytes)\n",
    "       40    0.000    0.000   30.621    0.766 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:429(_recv_bytes)\n",
    "       80    0.000    0.000   30.621    0.383 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/multiprocessing/connection.py:390(_recv)\n",
    "      280   30.618    0.109   30.618    0.109 {built-in method posix.read}\n",
    "     9743    2.374    0.000   24.489    0.003 /home/haoru/cs336/cs336-hw1/cs336_basics/train_bpe.py:106(merge_pair_and_update_counts)\n",
    "369218707   21.031    0.000   21.031    0.000 /home/haoru/cs336/cs336-hw1/cs336_basics/train_bpe.py:78(<lambda>)\n",
    "     9784    0.170    0.000   17.112    0.002 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/collections/__init__.py:673(update)\n",
    "     9744    0.012    0.000   16.841    0.002 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/collections/__init__.py:734(copy)\n",
    "     9745    0.025    0.000   16.829    0.002 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/collections/__init__.py:599(__init__)\n",
    "     9745   16.767    0.002   16.767    0.002 {function Counter.update at 0x7ce170c35f80}\n",
    "     9783    2.792    0.000    2.792    0.000 {method 'copy' of 'dict' objects}\n",
    "   277780    0.590    0.000    0.900    0.000 /home/haoru/cs336/cs336-hw1/cs336_basics/train_bpe.py:83(new_pretoken_after_merge_pair)\n",
    "  4068852    0.772    0.000    0.772    0.000 {method 'get' of 'dict' objects}\n",
    "  1457032    0.306    0.000    0.306    0.000 {method 'add' of 'set' objects}\n",
    "  1079666    0.276    0.000    0.276    0.000 {method 'discard' of 'set' objects}\n",
    "        1    0.145    0.145    0.249    0.249 /home/haoru/cs336/cs336-hw1/cs336_basics/train_bpe.py:41(build_pair_counts_and_index)\n",
    "  3641926    0.235    0.000    0.235    0.000 {built-in method builtins.len}\n",
    "  1081780    0.185    0.000    0.185    0.000 {method 'setdefault' of 'dict' objects}\n",
    "  1362718    0.152    0.000    0.152    0.000 {method 'keys' of 'dict' objects}\n",
    "  1369343    0.133    0.000    0.133    0.000 {method 'append' of 'list' objects}\n",
    "       40    0.002    0.000    0.115    0.003 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/multiprocessing/process.py:110(start)\n",
    "       40    0.114    0.003    0.114    0.003 {built-in method _pickle.loads}\n",
    "       40    0.001    0.000    0.109    0.003 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/multiprocessing/context.py:222(_Popen)\n",
    "       40    0.002    0.000    0.108    0.003 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/multiprocessing/context.py:279(_Popen)\n",
    "       40    0.001    0.000    0.106    0.003 /home/haoru/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/multiprocessing/popen_fork.py:16(__init__)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e166efb",
   "metadata": {},
   "source": [
    "## 2.5 Problem: BPE Training on OpenWebText (2 points)\n",
    "\n",
    "```(train_bpe_expts_owt)```\n",
    "\n",
    "***(a)*** Train a byte-level BPE tokenizer on the OpenWebText dataset, using a maximum vocabulary size of 32,000. Serialize the resulting vocabulary and merges to disk for further inspection. What is the longest token in the vocabulary? Does it make sense?\n",
    "\n",
    "***Resource requirements***: ≤ 12 hours (no GPUs), ≤ 100GB RAM \n",
    "\n",
    "**Answers**:\n",
    "- Training time: **4109.53s = 1.14 hour** (with inverted index optimization & heap)\n",
    "    \n",
    "    `\"data/owt_train.txt\"; vocab_size = 32000, num_processes = 40`\n",
    "- Longest token:\n",
    "\n",
    "    `id=25822, len=64 bytes, text='ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ'`\n",
    "\n",
    "***(b)*** Compare and contrast the tokenizer that you get training on TinyStories versus OpenWebText.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa85af",
   "metadata": {},
   "source": [
    "## 2.7 Problem: Experiments with tokenizers (4 points)\n",
    "\n",
    "`(tokenizer_experiments)`\n",
    "\n",
    "**(a)** Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyStories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these sampled documents into integer IDs. What is each tokenizer’s compression ratio (bytes/token)?\n",
    "\n",
    "**(b)** What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Compare the compression ratio and/or qualitatively describe what happens.\n",
    "\n",
    "\n",
    "**(c)** Estimate the throughput of your tokenizer (e.g., in bytes/second). How long would it take to tokenize the Pile dataset (825GB of text)?\n",
    "\n",
    "\n",
    "**(d)** Using your TinyStories and OpenWebText tokenizers, encode the respective training and development datasets into a sequence of integer token IDs. We’ll use this later to train our language model. We recommend serializing the token IDs as a NumPy array of datatype uint16. Why is uint16 an appropriate choice?\n",
    "\n",
    "\n",
    "### **Answers**\n",
    "\n",
    "**(a) & (b)**\n",
    "\n",
    "Used 10/10 documents for calculation...\n",
    "\n",
    "Compression ratio:\n",
    "- TinyStories tokenizer (10K vocab) on TinyStories docs: 4.191 bytes/token\n",
    "- OpenWebText tokenizer (32K vocab) on OpenWebText docs: 4.311 bytes/token\n",
    "\n",
    "Cross-domain comparison **(less efficient)**:\n",
    "- TinyStories tokenizer on OpenWebText docs: 3.270 bytes/token\n",
    "- OpenWebText tokenizer on TinyStories docs: 4.085 bytes/token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43f1a747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling documents...\n",
      "Sampled 10 TinyStories documents\n",
      "Sampled 10 OpenWebText documents\n",
      "\n",
      "Calculating compression ratios...\n",
      "    Used 10/10 documents for calculation\n",
      "TinyStories tokenizer (10K vocab) on TinyStories docs: 4.191 bytes/token\n",
      "    Used 10/10 documents for calculation\n",
      "OpenWebText tokenizer (32K vocab) on OpenWebText docs: 4.311 bytes/token\n",
      "    Used 10/10 documents for calculation\n",
      "    Used 10/10 documents for calculation\n",
      "\n",
      "Cross-domain comparison:\n",
      "TinyStories tokenizer on OpenWebText docs: 3.270 bytes/token\n",
      "OpenWebText tokenizer on TinyStories docs: 4.085 bytes/token\n"
     ]
    }
   ],
   "source": [
    "### Problem (tokenizer_experiments): Experiments with tokenizers\n",
    "\n",
    "import random\n",
    "from cs336_basics.tokenizer import Tokenizer\n",
    "\n",
    "def load_tokenizer_yaml(fname):\n",
    "    \"\"\"\n",
    "    Load vocab and merges from a YAML file, handling Python tuple tags safely.\n",
    "\n",
    "    Guarantees the 0..255 byte symbols exist (so no KeyError on non‑ASCII).\n",
    "    Uses any recoverable ASCII higher‑id tokens and merges.\n",
    "    Skips unrecoverable tokens/merges that contain �, which were destroyed on save.\n",
    "    \"\"\"\n",
    "    import yaml\n",
    "\n",
    "    # Custom constructor for Python tuples\n",
    "    yaml.SafeLoader.add_constructor('tag:yaml.org,2002:python/tuple',\n",
    "        lambda l, n: tuple(l.construct_sequence(n)))\n",
    "    \n",
    "    d = yaml.safe_load(open(fname, \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "    vocab = {i: bytes([i]) for i in range(256)}\n",
    "    for k, v in d[\"vocab\"].items():\n",
    "        i = int(k)\n",
    "        if i < 256:  # skip base bytes already added\n",
    "            continue\n",
    "\n",
    "        if isinstance(v, (list, tuple)):\n",
    "            b = bytes(v)\n",
    "        elif isinstance(v, str):\n",
    "            b = v.encode(\"utf-8\", \"ignore\")\n",
    "            # using .encode(\"utf-8\", \"ignore\") is equivalent to:\n",
    "            # if isinstance(v, str) and v.isascii():\n",
    "            #     b = v.encode(\"utf-8\")\n",
    "            # else:\n",
    "            #     b = b\"\"  # ignore non-ascii tokens\n",
    "            if b:  # skip empty bytes\n",
    "                vocab[i] = b\n",
    "    \n",
    "    merges = []\n",
    "    for a, b in d[\"merges\"]:\n",
    "        if isinstance(a, (list, tuple)): \n",
    "            merges.append((bytes(a), bytes(b)))\n",
    "        else:\n",
    "            merge_a, merge_b = a.encode(\"utf-8\", \"ignore\"), b.encode(\"utf-8\", \"ignore\")\n",
    "            if merge_a and merge_b:\n",
    "                merges.append((merge_a, merge_b))\n",
    "    return vocab, merges\n",
    "\n",
    "def sample_documents(file_path, num_samples=10, seed=42):\n",
    "    \"\"\"Sample documents from a dataset file separated by <|endoftext|>.\"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # For large files, read in chunks to avoid memory issues\n",
    "    documents = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        current_doc = \"\"\n",
    "        for line in f:\n",
    "            if '<|endoftext|>' in line:\n",
    "                # Split on <|endoftext|> and handle multiple occurrences per line\n",
    "                parts = line.split('<|endoftext|>')\n",
    "                current_doc += parts[0]\n",
    "                if current_doc.strip():\n",
    "                    documents.append(current_doc.strip())\n",
    "                \n",
    "                # Handle remaining parts\n",
    "                for part in parts[1:-1]:\n",
    "                    if part.strip():\n",
    "                        documents.append(part.strip())\n",
    "                \n",
    "                current_doc = parts[-1]\n",
    "                \n",
    "                # Stop if we have enough documents for sampling\n",
    "                if len(documents) >= num_samples * 10:  # Get more than needed for good sampling\n",
    "                    break\n",
    "            else:\n",
    "                current_doc += line\n",
    "    \n",
    "    # Add final document if exists\n",
    "    if current_doc.strip():\n",
    "        documents.append(current_doc.strip())\n",
    "    \n",
    "    # Sample random documents\n",
    "    sampled_docs = random.sample(documents, min(num_samples, len(documents)))\n",
    "    return sampled_docs\n",
    "\n",
    "def can_tokenize(text, tokenizer):\n",
    "    \"\"\"Check if a text can be tokenized without errors.\"\"\"\n",
    "    try:\n",
    "        tokenizer.encode(text)\n",
    "        return True\n",
    "    except KeyError as e:\n",
    "        print(f\"Doc failed; missing byte:\", e)\n",
    "        return False\n",
    "\n",
    "def calculate_compression_ratio(documents, tokenizer):\n",
    "    \"\"\"Calculate compression ratio (bytes/token) for a list of documents.\"\"\"\n",
    "    total_bytes = 0\n",
    "    total_tokens = 0\n",
    "    valid_docs = 0\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Skip documents that can't be tokenized\n",
    "        if not can_tokenize(doc, tokenizer):\n",
    "            continue\n",
    "            \n",
    "        # Count bytes (UTF-8 encoding)\n",
    "        doc_bytes = len(doc.encode('utf-8'))\n",
    "        total_bytes += doc_bytes\n",
    "        \n",
    "        # Count tokens\n",
    "        tokens = tokenizer.encode(doc)\n",
    "        total_tokens += len(tokens)\n",
    "        valid_docs += 1\n",
    "    \n",
    "    print(f\"    Used {valid_docs}/{len(documents)} documents for calculation\")\n",
    "    return total_bytes / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "# Load the trained tokenizers\n",
    "tinystories_vocab, tinystories_merges = load_tokenizer_yaml(\"artifacts/tinystories_bpe.yaml\")\n",
    "owt_vocab, owt_merges = load_tokenizer_yaml(\"artifacts/owt_bpe.yaml\")\n",
    "\n",
    "# Create tokenizer objects\n",
    "tinystories_tokenizer = Tokenizer(tinystories_vocab, tinystories_merges, special_tokens=[\"<|endoftext|>\"])\n",
    "owt_tokenizer = Tokenizer(owt_vocab, owt_merges, special_tokens=[\"<|endoftext|>\"])\n",
    "\n",
    "# Sample documents from both datasets\n",
    "print(\"Sampling documents...\")\n",
    "tinystories_docs = sample_documents(\"data/TinyStoriesV2-GPT4-train.txt\", num_samples=10)\n",
    "owt_docs = sample_documents(\"data/owt_train.txt\", num_samples=10)\n",
    "\n",
    "print(f\"Sampled {len(tinystories_docs)} TinyStories documents\")\n",
    "print(f\"Sampled {len(owt_docs)} OpenWebText documents\")\n",
    "\n",
    "# Calculate compression ratios\n",
    "print(\"\\nCalculating compression ratios...\")\n",
    "\n",
    "# TinyStories tokenizer on TinyStories documents\n",
    "ts_on_ts_ratio = calculate_compression_ratio(tinystories_docs, tinystories_tokenizer)\n",
    "print(f\"TinyStories tokenizer (10K vocab) on TinyStories docs: {ts_on_ts_ratio:.3f} bytes/token\")\n",
    "\n",
    "# OpenWebText tokenizer on OpenWebText documents  \n",
    "owt_on_owt_ratio = calculate_compression_ratio(owt_docs, owt_tokenizer)\n",
    "print(f\"OpenWebText tokenizer (32K vocab) on OpenWebText docs: {owt_on_owt_ratio:.3f} bytes/token\")\n",
    "\n",
    "# Cross-domain evaluation for comparison\n",
    "ts_on_owt_ratio = calculate_compression_ratio(owt_docs, tinystories_tokenizer)\n",
    "owt_on_ts_ratio = calculate_compression_ratio(tinystories_docs, owt_tokenizer)\n",
    "\n",
    "print(f\"\\nCross-domain comparison:\")\n",
    "print(f\"TinyStories tokenizer on OpenWebText docs: {ts_on_owt_ratio:.3f} bytes/token\")\n",
    "print(f\"OpenWebText tokenizer on TinyStories docs: {owt_on_ts_ratio:.3f} bytes/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8146f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding OpenWebText dataset...\n",
      "Processing chunk 1/4, size: 2954033787 chars\n",
      "Processing chunk 2/4, size: 2953974483 chars\n",
      "Processing chunk 3/4, size: 2953958612 chars\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "from cs336_basics.tokenizer import Tokenizer\n",
    "from cs336_basics.pretokenization import find_chunk_boundaries\n",
    "\n",
    "def encode_worker(start: int, end: int, input_path: str, tokenizer, q: Queue):\n",
    "    \"\"\"\n",
    "    Worker function to encode a chunk of the file\n",
    "    \"\"\"\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        # Encode chunk and put tokens in queue\n",
    "        tokens = []\n",
    "        for token_id in tokenizer.encode_iterable([chunk]):\n",
    "            tokens.append(token_id)\n",
    "        q.put(tokens)\n",
    "\n",
    "def encode_dataset_parallel(input_path, output_path, tokenizer, num_chunks):\n",
    "    \"\"\"\n",
    "    Encode dataset using chunking approach from pretokenization.py and\n",
    "    parallel processing following run_train_bpe() pattern.\n",
    "    \"\"\"\n",
    "    print(f\"Starting parallel encoding with {num_chunks} processes\")\n",
    "\n",
    "    # Create processes and queue (same pattern as run_train_bpe)\n",
    "    processes = []\n",
    "    q = Queue()\n",
    "    \n",
    "    with open(input_path, \"rb\") as f:\n",
    "        # Use the same chunking logic as pretokenization.py\n",
    "        boundaries = find_chunk_boundaries(f, num_chunks, b\"<|endoftext|>\")\n",
    "        \n",
    "        # Process each chunk separately\n",
    "        for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "            p = Process(target=encode_worker, args=(start, end, input_path, tokenizer, q))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "    \n",
    "        # Collect and merge tokens from workers\n",
    "        all_tokens = []\n",
    "        for _ in range(len(processes)):\n",
    "            all_tokens.extend(q.get())\n",
    "    \n",
    "        # Wait for all processes to complete\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        \n",
    "        # Convert to uint16 numpy array\n",
    "        tokens_array = np.array(all_tokens, dtype=np.uint16)\n",
    "        np.save(output_path, tokens_array)\n",
    "        print(f\"Saved {len(tokens_array)} tokens to {output_path}\")\n",
    "        return tokens_array\n",
    "\n",
    "# Load tokenizers\n",
    "# tinystories_vocab, tinystories_merges = load_tokenizer_yaml(\"artifacts/tinystories_bpe.yaml\")\n",
    "owt_vocab, owt_merges = load_tokenizer_yaml(\"artifacts/owt_bpe.yaml\")\n",
    "\n",
    "# ts_tokenizer = Tokenizer(tinystories_vocab, tinystories_merges, special_tokens=[\"<|endoftext|>\"])\n",
    "owt_tokenizer = Tokenizer(owt_vocab, owt_merges, special_tokens=[\"<|endoftext|>\"])\n",
    "\n",
    "# Encode datasets\n",
    "# print(\"Encoding TinyStories dataset...\")\n",
    "# encode_dataset_parallel(\"data/TinyStoriesV2-GPT4-train.txt\", \"data/tinystories_train_tokens.npy\", ts_tokenizer)\n",
    "\n",
    "print(\"Encoding OpenWebText dataset...\")\n",
    "encode_dataset_parallel(\"data/owt_train.txt\", \"data/owt_train_tokens.npy\", owt_tokenizer, num_chunks=100)\n",
    "\n",
    "\"\"\"\n",
    "23m 25.6s\n",
    "Encoding TinyStories dataset...\n",
    "Processing chunk 1/4, size: 556710563 chars\n",
    "Processing chunk 2/4, size: 556712694 chars\n",
    "Processing chunk 3/4, size: 556712530 chars\n",
    "Processing chunk 4/4, size: 556709481 chars\n",
    "Saved 542447487 tokens to data/tinystories_train_tokens.npy\n",
    "array([ 10, 430, 439, ..., 317,  89, 111],\n",
    "      shape=(542447487,), dtype=uint16)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv: cs336)",
   "language": "python",
   "name": "cs336-uv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
