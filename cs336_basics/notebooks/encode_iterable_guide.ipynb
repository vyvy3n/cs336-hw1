{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ Iterable, Iterator –∏ encode_iterable\n",
        "\n",
        "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ –æ–±—ä—è—Å–Ω—è–µ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ **Iterable** –∏ **Iterator** –≤ Python –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –≥–¥–µ –º–æ–∂–µ—Ç –ø—Ä–∏–≥–æ–¥–∏—Ç—å—Å—è –º–µ—Ç–æ–¥ `encode_iterable` –∏–∑ –∫–ª–∞—Å—Å–∞ `Tokenizer`.\n",
        "\n",
        "## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n",
        "1. [–û—Å–Ω–æ–≤—ã Iterable –∏ Iterator](#basics)\n",
        "2. [–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –∏ –ª–µ–Ω–∏–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è](#generators)\n",
        "3. [–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã encode_iterable](#examples)\n",
        "4. [–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞–º—è—Ç–∏](#memory)\n",
        "5. [–ü–∞–π–ø–ª–∞–π–Ω—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö](#pipelines)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. –û—Å–Ω–æ–≤—ã Iterable –∏ Iterator {#basics}\n",
        "\n",
        "### –ß—Ç–æ —Ç–∞–∫–æ–µ Iterable?\n",
        "**Iterable** (–∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–π) - —ç—Ç–æ –æ–±—ä–µ–∫—Ç, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –º–æ–∂–Ω–æ –∏—Ç–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è (–ø—Ä–æ—Ö–æ–¥–∏—Ç—å —ç–ª–µ–º–µ–Ω—Ç –∑–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–º). –í—Å–µ iterable –æ–±—ä–µ–∫—Ç—ã –∏–º–µ—é—Ç –º–µ—Ç–æ–¥ `__iter__()`, –∫–æ—Ç–æ—Ä—ã–π –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç iterator.\n",
        "\n",
        "### –ß—Ç–æ —Ç–∞–∫–æ–µ Iterator?\n",
        "**Iterator** (–∏—Ç–µ—Ä–∞—Ç–æ—Ä) - —ç—Ç–æ –æ–±—ä–µ–∫—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ –æ–¥–Ω–æ–º—É –∑–∞ —Ä–∞–∑. –£ –Ω–µ–≥–æ –µ—Å—Ç—å –º–µ—Ç–æ–¥—ã:\n",
        "- `__iter__()` - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∞–º —Å–µ–±—è\n",
        "- `__next__()` - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç –∏–ª–∏ –ø–æ–¥–Ω–∏–º–∞–µ—Ç `StopIteration`\n",
        "\n",
        "### –û—Å–Ω–æ–≤–Ω—ã–µ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–µ –æ–±—ä–µ–∫—Ç—ã:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "–°–ø–∏—Å–æ–∫: [1, 2, 3, 4, 5]\n",
            "–°—Ç—Ä–æ–∫–∞: ['–ü', '—Ä', '–∏', '–≤', '–µ', '—Ç']\n",
            "–ö–ª—é—á–∏ —Å–ª–æ–≤–∞—Ä—è: ['a', 'b', 'c']\n",
            "–ó–Ω–∞—á–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è: [1, 2, 3]\n",
            "\n",
            "–°–ø–∏—Å–æ–∫ –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–π: True\n",
            "–°—Ç—Ä–æ–∫–∞ –∏—Ç–µ—Ä–∏—Ä—É–µ–º–∞—è: True\n",
            "–°–ª–æ–≤–∞—Ä—å –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–π: True\n"
          ]
        }
      ],
      "source": [
        "# –ü—Ä–∏–º–µ—Ä—ã –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤\n",
        "my_list = [1, 2, 3, 4, 5]\n",
        "my_string = \"–ü—Ä–∏–≤–µ—Ç\"\n",
        "my_dict = {'a': 1, 'b': 2, 'c': 3}\n",
        "\n",
        "print(\"–°–ø–∏—Å–æ–∫:\", list(my_list))\n",
        "print(\"–°—Ç—Ä–æ–∫–∞:\", list(my_string))\n",
        "print(\"–ö–ª—é—á–∏ —Å–ª–æ–≤–∞—Ä—è:\", list(my_dict.keys()))\n",
        "print(\"–ó–Ω–∞—á–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è:\", list(my_dict.values()))\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∏–º, —è–≤–ª—è—é—Ç—Å—è –ª–∏ –æ–Ω–∏ –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–º–∏\n",
        "print(f\"\\n–°–ø–∏—Å–æ–∫ –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–π: {hasattr(my_list, '__iter__')}\")\n",
        "print(f\"–°—Ç—Ä–æ–∫–∞ –∏—Ç–µ—Ä–∏—Ä—É–µ–º–∞—è: {hasattr(my_string, '__iter__')}\")\n",
        "print(f\"–°–ª–æ–≤–∞—Ä—å –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–π: {hasattr(my_dict, '__iter__')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "–¢–∏–ø –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞: <class 'list_iterator'>\n",
            "–£ –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞ –µ—Å—Ç—å __iter__: True\n",
            "–£ –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞ –µ—Å—Ç—å __next__: True\n",
            "\n",
            "–†—É—á–Ω–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è:\n",
            "–ü–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç: 10\n",
            "–í—Ç–æ—Ä–æ–π —ç–ª–µ–º–µ–Ω—Ç: 20\n",
            "–¢—Ä–µ—Ç–∏–π —ç–ª–µ–º–µ–Ω—Ç: 30\n",
            "–ß–µ—Ç–≤—ë—Ä—Ç—ã–π —ç–ª–µ–º–µ–Ω—Ç: 40\n",
            "–î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞ - –ø–æ–¥–Ω—è—Ç–æ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ StopIteration\n"
          ]
        }
      ],
      "source": [
        "# –ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞ –∏ —Ä–∞–±–æ—Ç–∞ —Å –Ω–∏–º\n",
        "my_list = [10, 20, 30, 40]\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º –∏—Ç–µ—Ä–∞—Ç–æ—Ä –∏–∑ —Å–ø–∏—Å–∫–∞\n",
        "list_iterator = iter(my_list)\n",
        "print(f\"–¢–∏–ø –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞: {type(list_iterator)}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–µ—Ç–æ–¥–æ–≤ –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞\n",
        "print(f\"–£ –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞ –µ—Å—Ç—å __iter__: {hasattr(list_iterator, '__iter__')}\")\n",
        "print(f\"–£ –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞ –µ—Å—Ç—å __next__: {hasattr(list_iterator, '__next__')}\")\n",
        "\n",
        "# –†—É—á–Ω–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é next()\n",
        "print(\"\\n–†—É—á–Ω–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è:\")\n",
        "print(f\"–ü–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç: {next(list_iterator)}\")\n",
        "print(f\"–í—Ç–æ—Ä–æ–π —ç–ª–µ–º–µ–Ω—Ç: {next(list_iterator)}\")\n",
        "print(f\"–¢—Ä–µ—Ç–∏–π —ç–ª–µ–º–µ–Ω—Ç: {next(list_iterator)}\")\n",
        "\n",
        "# –ú–æ–∂–µ–º –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∏—Ç–µ—Ä–∞—Ü–∏—é\n",
        "print(f\"–ß–µ—Ç–≤—ë—Ä—Ç—ã–π —ç–ª–µ–º–µ–Ω—Ç: {next(list_iterator)}\")\n",
        "\n",
        "# –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç –≤—ã–∑–æ–≤–µ—Ç StopIteration\n",
        "try:\n",
        "    print(f\"–ü—è—Ç—ã–π —ç–ª–µ–º–µ–Ω—Ç: {next(list_iterator)}\")\n",
        "except StopIteration:\n",
        "    print(\"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –∫–æ–Ω–µ—Ü –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞ - –ø–æ–¥–Ω—è—Ç–æ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ StopIteration\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –∏ –ª–µ–Ω–∏–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è {#generators}\n",
        "\n",
        "**–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã** - —ç—Ç–æ —É–¥–æ–±–Ω—ã–π —Å–ø–æ—Å–æ–± —Å–æ–∑–¥–∞–Ω–∏—è –∏—Ç–µ—Ä–∞—Ç–æ—Ä–æ–≤ –≤ Python. –û–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ `yield` –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–µ–∞–ª–∏–∑—É—é—Ç –ø—Ä–æ—Ç–æ–∫–æ–ª –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞.\n",
        "\n",
        "### –ì–ª–∞–≤–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤:\n",
        "- **–õ–µ–Ω–∏–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è** - –∑–Ω–∞—á–µ–Ω–∏—è –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ –∑–∞–ø—Ä–æ—Å—É\n",
        "- **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞–º—è—Ç–∏** - –Ω–µ –Ω—É–∂–Ω–æ —Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ\n",
        "- **–ü—Ä–æ—Å—Ç–æ—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è** - –Ω–µ –Ω—É–∂–Ω–æ –ø–∏—Å–∞—Ç—å –∫–ª–∞—Å—Å—ã —Å `__iter__` –∏ `__next__`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü—Ä–∏–º–µ—Ä –ø—Ä–æ—Å—Ç–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞\n",
        "def simple_generator():\n",
        "    \"\"\"–ü—Ä–æ—Å—Ç–æ–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å yield\"\"\"\n",
        "    print(\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º 1\")\n",
        "    yield 1\n",
        "    print(\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º 2\") \n",
        "    yield 2\n",
        "    print(\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º 3\")\n",
        "    yield 3\n",
        "    print(\"–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∑–∞–≤–µ—Ä—à—ë–Ω\")\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä (–ø–æ–∫–∞ –Ω–∏—á–µ–≥–æ –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è)\n",
        "gen = simple_generator()\n",
        "print(f\"–¢–∏–ø –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞: {type(gen)}\")\n",
        "print(\"–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω, –Ω–æ –∫–æ–¥ –µ—â—ë –Ω–µ –≤—ã–ø–æ–ª–Ω–∏–ª—Å—è\\n\")\n",
        "\n",
        "# –¢–µ–ø–µ—Ä—å –∑–∞–ø—É—Å–∫–∞–µ–º –∏—Ç–µ—Ä–∞—Ü–∏—é\n",
        "print(\"–ù–∞—á–∏–Ω–∞–µ–º –∏—Ç–µ—Ä–∞—Ü–∏—é:\")\n",
        "for value in gen:\n",
        "    print(f\"–ü–æ–ª—É—á–∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ: {value}\")\n",
        "    print(\"---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ª–µ–Ω–∏–≤—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π vs –∂–∞–¥–Ω—ã—Ö\n",
        "def expensive_operation(x):\n",
        "    \"\"\"–ò–º–∏—Ç–∏—Ä—É–µ–º –¥–æ—Ä–æ–≥—É—é –æ–ø–µ—Ä–∞—Ü–∏—é\"\"\"\n",
        "    print(f\"–í—ã–ø–æ–ª–Ω—è–µ–º –¥–æ—Ä–æ–≥—É—é –æ–ø–µ—Ä–∞—Ü–∏—é –¥–ª—è {x}\")\n",
        "    return x ** 2\n",
        "\n",
        "print(\"=== –ñ–ê–î–ù–´–ô –ü–û–î–•–û–î (List Comprehension) ===\")\n",
        "print(\"–°–æ–∑–¥–∞—ë–º —Å–ø–∏—Å–æ–∫ - –≤—Å–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–∞–∑—É:\")\n",
        "eager_results = [expensive_operation(x) for x in range(5)]\n",
        "print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç: {eager_results}\")\n",
        "print()\n",
        "\n",
        "print(\"=== –õ–ï–ù–ò–í–´–ô –ü–û–î–•–û–î (Generator Expression) ===\")\n",
        "print(\"–°–æ–∑–¥–∞—ë–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä - –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –æ—Ç–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è:\")\n",
        "lazy_results = (expensive_operation(x) for x in range(5))\n",
        "print(f\"–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω: {lazy_results}\")\n",
        "print(\"–ù–∏–∫–∞–∫–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –ø–æ–∫–∞ –Ω–µ –±—ã–ª–æ!\")\n",
        "print()\n",
        "\n",
        "print(\"–¢–µ–ø–µ—Ä—å –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 2 —ç–ª–µ–º–µ–Ω—Ç–∞:\")\n",
        "for i, result in enumerate(lazy_results):\n",
        "    if i >= 2:\n",
        "        break\n",
        "    print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç {i}: {result}\")\n",
        "\n",
        "print(\"\\n–û—Å—Ç–∞–ª—å–Ω—ã–µ 3 —ç–ª–µ–º–µ–Ω—Ç–∞ –Ω–µ –±—ã–ª–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã - —ç–∫–æ–Ω–æ–º–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã encode_iterable {#examples}\n",
        "\n",
        "–¢–µ–ø–µ—Ä—å —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º, –≥–¥–µ –º–æ–∂–µ—Ç –ø—Ä–∏–≥–æ–¥–∏—Ç—å—Å—è –º–µ—Ç–æ–¥ `encode_iterable` –∏–∑ –≤–∞—à–µ–≥–æ –∫–ª–∞—Å—Å–∞ `Tokenizer`. –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∏—Ç–µ—Ä–∏—Ä—É–µ–º—ã–π –æ–±—ä–µ–∫—Ç —Å—Ç—Ä–æ–∫ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ç–µ—Ä–∞—Ç–æ—Ä —Ç–æ–∫–µ–Ω–æ–≤.\n",
        "\n",
        "### –°–∏–≥–Ω–∞—Ç—É—Ä–∞ –º–µ—Ç–æ–¥–∞:\n",
        "```python\n",
        "def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
        "    for text in iterable:\n",
        "        for token_id in self.encode(text):\n",
        "            yield token_id\n",
        "```\n",
        "\n",
        "### –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü—Ä–∏–º–µ—Ä 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–æ–≥–æ —Ñ–∞–π–ª–∞ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ\n",
        "import os\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º —Å—Ç—Ä–æ–∫\n",
        "print(\"–°–æ–∑–¥–∞—ë–º –±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª...\")\n",
        "with open('large_text.txt', 'w', encoding='utf-8') as f:\n",
        "    for i in range(1000):\n",
        "        f.write(f\"–≠—Ç–æ —Å—Ç—Ä–æ–∫–∞ –Ω–æ–º–µ—Ä {i}. –ó–¥–µ—Å—å –º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.\\n\")\n",
        "\n",
        "print(\"–§–∞–π–ª —Å–æ–∑–¥–∞–Ω!\")\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ (–≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä)\n",
        "def read_file_lines(filename):\n",
        "    \"\"\"–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —á–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª –ø–æ—Å—Ç—Ä–æ—á–Ω–æ\"\"\"\n",
        "    print(f\"–û—Ç–∫—Ä—ã–≤–∞–µ–º —Ñ–∞–π–ª {filename}\")\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        for line_no, line in enumerate(f, 1):\n",
        "            yield line.strip()\n",
        "\n",
        "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ª–µ–Ω–∏–≤–æ–≥–æ —á—Ç–µ–Ω–∏—è\n",
        "lines_generator = read_file_lines('large_text.txt')\n",
        "print(f\"–¢–∏–ø –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞: {type(lines_generator)}\")\n",
        "\n",
        "# –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏\n",
        "print(\"\\n–ü–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞:\")\n",
        "for i, line in enumerate(lines_generator):\n",
        "    if i >= 3:\n",
        "        break\n",
        "    print(f\"–°—Ç—Ä–æ–∫–∞ {i+1}: {line}\")\n",
        "\n",
        "print(\"\\n–û—Å—Ç–∞–ª—å–Ω—ã–µ 997 —Å—Ç—Ä–æ–∫ –Ω–µ –±—ã–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –ø–∞–º—è—Ç—å!\")\n",
        "\n",
        "# –ó–¥–µ—Å—å –º—ã –±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ encode_iterable:\n",
        "# tokenizer = Tokenizer.from_files('vocab.txt', 'merges.txt')\n",
        "# token_stream = tokenizer.encode_iterable(lines_generator)\n",
        "\n",
        "# –û—á–∏—Å—Ç–∫–∞\n",
        "os.remove('large_text.txt')\n",
        "print(\"–§–∞–π–ª —É–¥–∞–ª—ë–Ω\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü—Ä–∏–º–µ—Ä 2: –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "import time\n",
        "\n",
        "def data_stream_simulator():\n",
        "    \"\"\"–°–∏–º—É–ª–∏—Ä—É–µ–º –ø–æ—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ —á–∞—Ç–∞ –∏–ª–∏ —Å–µ—Ç–∏)\"\"\"\n",
        "    messages = [\n",
        "        \"–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\",\n",
        "        \"–°–µ–≥–æ–¥–Ω—è —Ö–æ—Ä–æ—à–∞—è –ø–æ–≥–æ–¥–∞\",\n",
        "        \"–ó–∞–≤—Ç—Ä–∞ –ø–æ–π–¥—ë–º –≤ –∫–∏–Ω–æ\",\n",
        "        \"–ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ Python\",\n",
        "        \"–ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ\"\n",
        "    ]\n",
        "    \n",
        "    for i, msg in enumerate(messages):\n",
        "        print(f\"üì• –ü–æ–ª—É—á–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ {i+1}: {msg}\")\n",
        "        # –ò–º–∏—Ç–∏—Ä—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
        "        time.sleep(0.5)\n",
        "        yield msg\n",
        "\n",
        "print(\"=== –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π ===\")\n",
        "print(\"–°–æ–æ–±—â–µ–Ω–∏—è –ø–æ—Å—Ç—É–ø–∞—é—Ç –ø–æ –æ–¥–Ω–æ–º—É...\")\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º –ø–æ—Ç–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π\n",
        "message_stream = data_stream_simulator()\n",
        "\n",
        "# –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤:\n",
        "# tokenizer = Tokenizer.from_files('vocab.txt', 'merges.txt') \n",
        "# token_stream = tokenizer.encode_iterable(message_stream)\n",
        "\n",
        "# –ò–º–∏—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–æ—Ç–æ–∫–∞\n",
        "print(\"\\nüîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ –º–µ—Ä–µ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è:\")\n",
        "for i, message in enumerate(message_stream):\n",
        "    print(f\"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: '{message}' (–¥–ª–∏–Ω–∞: {len(message)} —Å–∏–º–≤–æ–ª–æ–≤)\")\n",
        "    if i >= 2:  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 —Å–æ–æ–±—â–µ–Ω–∏—è\n",
        "        print(\"‚èπÔ∏è  –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏...\")\n",
        "        break\n",
        "\n",
        "print(\"\\n–û—Å—Ç–∞–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –º–æ–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ–∑–∂–µ –∏–ª–∏ –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –≤–æ–≤—Å–µ!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞–º—è—Ç–∏ {#memory}\n",
        "\n",
        "–û–¥–Ω–æ –∏–∑ –≥–ª–∞–≤–Ω—ã—Ö –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ `encode_iterable` - **—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞–º—è—Ç–∏**. –í–º–µ—Å—Ç–æ –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö —Å—Ä–∞–∑—É, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø–æ —ç–ª–µ–º–µ–Ω—Ç–∞–º.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–∞–º—è—Ç–∏\n",
        "import sys\n",
        "\n",
        "print(\"=== –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ ===\")\n",
        "\n",
        "# 1. –ù–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± - –∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å—ë –≤ –ø–∞–º—è—Ç—å\n",
        "def create_large_list(size):\n",
        "    \"\"\"–°–æ–∑–¥–∞—ë–º –±–æ–ª—å—à–æ–π —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫\"\"\"\n",
        "    return [f\"–°—Ç—Ä–æ–∫–∞ –Ω–æ–º–µ—Ä {i} —Å –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\" for i in range(size)]\n",
        "\n",
        "# 2. –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Å–ø–æ—Å–æ–± - –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä\n",
        "def create_large_generator(size):\n",
        "    \"\"\"–°–æ–∑–¥–∞—ë–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å—Ç—Ä–æ–∫\"\"\"\n",
        "    for i in range(size):\n",
        "        yield f\"–°—Ç—Ä–æ–∫–∞ –Ω–æ–º–µ—Ä {i} —Å –¥–ª–∏–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\"\n",
        "\n",
        "# –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å 100,000 —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n",
        "size = 100_000\n",
        "\n",
        "print(f\"–°–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –∏–∑ {size:,} —Å—Ç—Ä–æ–∫...\")\n",
        "\n",
        "# –ò–∑–º–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Å–ø–∏—Å–∫–∞\n",
        "large_list = create_large_list(size)\n",
        "list_size = sys.getsizeof(large_list)\n",
        "print(f\"üì¶ –†–∞–∑–º–µ—Ä —Å–ø–∏—Å–∫–∞: {list_size:,} –±–∞–π—Ç ({list_size / 1024 / 1024:.1f} –ú–ë)\")\n",
        "\n",
        "# –ò–∑–º–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞\n",
        "large_generator = create_large_generator(size)\n",
        "generator_size = sys.getsizeof(large_generator)\n",
        "print(f\"‚ö° –†–∞–∑–º–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞: {generator_size:,} –±–∞–π—Ç ({generator_size / 1024:.1f} –ö–ë)\")\n",
        "\n",
        "# –í—ã—á–∏—Å–ª—è–µ–º —ç–∫–æ–Ω–æ–º–∏—é\n",
        "savings_ratio = list_size / generator_size\n",
        "print(f\"üíæ –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏: {savings_ratio:.0f}x —Ä–∞–∑!\")\n",
        "\n",
        "print(f\"\\nüìä –°–ø–∏—Å–æ–∫ –∑–∞–Ω–∏–º–∞–µ—Ç {list_size:,} –±–∞–π—Ç\")\n",
        "print(f\"üìä –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∑–∞–Ω–∏–º–∞–µ—Ç {generator_size:,} –±–∞–π—Ç\")\n",
        "print(f\"üéØ –†–∞–∑–Ω–∏—Ü–∞: {list_size - generator_size:,} –±–∞–π—Ç\")\n",
        "\n",
        "# –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
        "del large_list\n",
        "print(\"\\n‚úÖ –°–ø–∏—Å–æ–∫ —É–¥–∞–ª—ë–Ω –∏–∑ –ø–∞–º—è—Ç–∏\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. –ü–∞–π–ø–ª–∞–π–Ω—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö {#pipelines}\n",
        "\n",
        "–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –æ—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è **–ø–∞–π–ø–ª–∞–π–Ω–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö** - —Ü–µ–ø–æ—á–µ–∫ —Ñ—É–Ω–∫—Ü–∏–π, –≥–¥–µ –∫–∞–∂–¥–∞—è —Å—Ç–∞–¥–∏—è –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏ –ø–µ—Ä–µ–¥–∞—ë—Ç –∏—Ö –¥–∞–ª—å—à–µ.\n",
        "\n",
        "–° `encode_iterable` –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –ø–∞–π–ø–ª–∞–π–Ω—ã –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü—Ä–∏–º–µ—Ä –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö\n",
        "import re\n",
        "\n",
        "def data_source():\n",
        "    \"\"\"–ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö - —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã\"\"\"\n",
        "    texts = [\n",
        "        \"   –ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!   \",\n",
        "        \"–≠—Ç–æ –æ—á–µ–Ω—å –î–õ–ò–ù–ù–´–ô —Ç–µ–∫—Å—Ç —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º —Å–ª–æ–≤\",\n",
        "        \"–∫–æ—Ä–æ—Ç–∫–∏–π\",\n",
        "        \"–¢–ï–ö–°–¢ –í –í–ï–†–•–ù–ï–ú –†–ï–ì–ò–°–¢–†–ï!!!\",\n",
        "        \"   \\t\\n   –ø—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç   \\n\\t   \",\n",
        "        \"–¢–µ–∫—Å—Ç —Å —Ü–∏—Ñ—Ä–∞–º–∏ 123 –∏ —Å–∏–º–≤–æ–ª–∞–º–∏ @#$%\",\n",
        "        \"–ï—â—ë –æ–¥–∏–Ω –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏\"\n",
        "    ]\n",
        "    for text in texts:\n",
        "        yield text\n",
        "\n",
        "def clean_whitespace(texts):\n",
        "    \"\"\"–°—Ç–∞–¥–∏—è 1: –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤\"\"\"\n",
        "    for text in texts:\n",
        "        cleaned = text.strip()\n",
        "        if cleaned:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
        "            yield cleaned\n",
        "\n",
        "def normalize_case(texts):\n",
        "    \"\"\"–°—Ç–∞–¥–∏—è 2: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–≥–∏—Å—Ç—Ä–∞\"\"\"\n",
        "    for text in texts:\n",
        "        yield text.lower()\n",
        "\n",
        "def filter_by_length(texts, min_length=5, max_length=50):\n",
        "    \"\"\"–°—Ç–∞–¥–∏—è 3: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–ª–∏–Ω–µ\"\"\"\n",
        "    for text in texts:\n",
        "        if min_length <= len(text) <= max_length:\n",
        "            yield text\n",
        "\n",
        "def remove_special_chars(texts):\n",
        "    \"\"\"–°—Ç–∞–¥–∏—è 4: –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤\"\"\"\n",
        "    for text in texts:\n",
        "        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –ø—Ä–æ–±–µ–ª—ã\n",
        "        cleaned = re.sub(r'[^–∞-—è—ë\\w\\s]', '', text, flags=re.IGNORECASE)\n",
        "        yield cleaned\n",
        "\n",
        "def add_prefix(texts, prefix=\"[–û–ë–†–ê–ë–û–¢–ê–ù–û]\"):\n",
        "    \"\"\"–°—Ç–∞–¥–∏—è 5: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å–∞\"\"\"\n",
        "    for text in texts:\n",
        "        yield f\"{prefix} {text}\"\n",
        "\n",
        "# –°–æ–∑–¥–∞—ë–º –ø–∞–π–ø–ª–∞–π–Ω\n",
        "print(\"=== –ü–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ ===\")\n",
        "print(\"–ò—Å—Ç–æ—á–Ω–∏–∫ -> –û—á–∏—Å—Ç–∫–∞ -> –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è -> –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è -> –£–¥–∞–ª–µ–Ω–∏–µ —Å–∏–º–≤–æ–ª–æ–≤ -> –ü—Ä–µ—Ñ–∏–∫—Å -> –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\")\n",
        "print()\n",
        "\n",
        "# –°–æ–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å—Ç–∞–¥–∏–∏\n",
        "source = data_source()\n",
        "stage1 = clean_whitespace(source)\n",
        "stage2 = normalize_case(stage1)\n",
        "stage3 = filter_by_length(stage2, min_length=10, max_length=60)\n",
        "stage4 = remove_special_chars(stage3)\n",
        "stage5 = add_prefix(stage4)\n",
        "\n",
        "# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω\n",
        "print(\"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞:\")\n",
        "processed_texts = list(stage5)\n",
        "for i, text in enumerate(processed_texts, 1):\n",
        "    print(f\"{i}. {text}\")\n",
        "\n",
        "print(f\"\\n–ò–∑ 7 –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ–ª—É—á–∏–ª–∏ {len(processed_texts)} –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö\")\n",
        "\n",
        "# –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤:\n",
        "# tokenizer = Tokenizer.from_files('vocab.txt', 'merges.txt')\n",
        "# –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
        "final_pipeline = add_prefix(remove_special_chars(filter_by_length(normalize_case(clean_whitespace(data_source())))))\n",
        "# token_stream = tokenizer.encode_iterable(final_pipeline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü—Ä–∏–º–µ—Ä —Å itertools - –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Ä–∞–±–æ—Ç–∞ —Å –∏—Ç–µ—Ä–∞—Ç–æ—Ä–∞–º–∏\n",
        "import itertools\n",
        "\n",
        "def infinite_text_generator():\n",
        "    \"\"\"–ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞\"\"\"\n",
        "    templates = [\n",
        "        \"–°–æ–æ–±—â–µ–Ω–∏–µ –Ω–æ–º–µ—Ä {}\",\n",
        "        \"–î–æ–∫—É–º–µ–Ω—Ç –ø–æ–¥ –Ω–æ–º–µ—Ä–æ–º {}\",\n",
        "        \"–¢–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {}\",\n",
        "        \"–°—Ç—Ä–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö ‚Ññ {}\"\n",
        "    ]\n",
        "    counter = 1\n",
        "    while True:\n",
        "        template = templates[counter % len(templates)]\n",
        "        yield template.format(counter)\n",
        "        counter += 1\n",
        "\n",
        "print(\"=== –†–∞–±–æ—Ç–∞ —Å –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ ===\")\n",
        "\n",
        "# 1. –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –±–µ—Å–∫–æ–Ω–µ—á–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
        "infinite_texts = infinite_text_generator()\n",
        "limited_texts = itertools.islice(infinite_texts, 5)  # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5\n",
        "\n",
        "print(\"–ü–µ—Ä–≤—ã–µ 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏–∑ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:\")\n",
        "for text in limited_texts:\n",
        "    print(f\"- {text}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# 2. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "def categorized_texts():\n",
        "    \"\"\"–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏\"\"\"\n",
        "    data = [\n",
        "        (\"–Ω–æ–≤–æ—Å—Ç–∏\", \"–°–µ–≥–æ–¥–Ω—è —Ö–æ—Ä–æ—à–∞—è –ø–æ–≥–æ–¥–∞\"),\n",
        "        (\"–Ω–æ–≤–æ—Å—Ç–∏\", \"–ó–∞–≤—Ç—Ä–∞ –±—É–¥–µ—Ç –¥–æ–∂–¥—å\"),\n",
        "        (\"—Ç–µ—Ö–Ω–∏–∫–∞\", \"–ù–æ–≤—ã–π iPhone –≤—ã–ø—É—â–µ–Ω\"),\n",
        "        (\"—Ç–µ—Ö–Ω–∏–∫–∞\", \"Python 3.12 –¥–æ—Å—Ç—É–ø–µ–Ω\"),\n",
        "        (\"—Å–ø–æ—Ä—Ç\", \"–§—É—Ç–±–æ–ª—å–Ω—ã–π –º–∞—Ç—á –∑–∞–≤–µ—Ä—à—ë–Ω\"),\n",
        "        (\"—Å–ø–æ—Ä—Ç\", \"–¢–µ–Ω–Ω–∏—Å–Ω—ã–π —Ç—É—Ä–Ω–∏—Ä –Ω–∞—á–∞–ª—Å—è\")\n",
        "    ]\n",
        "    for category, text in data:\n",
        "        yield category, text\n",
        "\n",
        "print(\"=== –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º ===\")\n",
        "texts_with_categories = categorized_texts()\n",
        "\n",
        "# –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n",
        "grouped = itertools.groupby(texts_with_categories, key=lambda x: x[0])\n",
        "\n",
        "for category, group in grouped:\n",
        "    print(f\"üìÇ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}\")\n",
        "    texts_in_category = [text for _, text in group]\n",
        "    for text in texts_in_category:\n",
        "        print(f\"  - {text}\")\n",
        "    \n",
        "    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å encode_iterable –∫ —Ç–µ–∫—Å—Ç–∞–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:\n",
        "    # token_stream = tokenizer.encode_iterable(texts_in_category)\n",
        "    print()\n",
        "\n",
        "# 3. –¶–µ–ø–æ—á–∫–∞ –∏—Ç–µ—Ä–∞—Ç–æ—Ä–æ–≤\n",
        "print(\"=== –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö ===\")\n",
        "\n",
        "def source1():\n",
        "    yield \"–ü–µ—Ä–≤—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: –¥–æ–∫—É–º–µ–Ω—Ç 1\"\n",
        "    yield \"–ü–µ—Ä–≤—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: –¥–æ–∫—É–º–µ–Ω—Ç 2\"\n",
        "\n",
        "def source2():\n",
        "    yield \"–í—Ç–æ—Ä–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫: —Ñ–∞–π–ª A\"\n",
        "    yield \"–í—Ç–æ—Ä–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫: —Ñ–∞–π–ª B\"\n",
        "\n",
        "def source3():\n",
        "    yield \"–¢—Ä–µ—Ç–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫: –∑–∞–ø–∏—Å—å X\"\n",
        "    yield \"–¢—Ä–µ—Ç–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫: –∑–∞–ø–∏—Å—å Y\"\n",
        "\n",
        "# –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ –æ–¥–∏–Ω –ø–æ—Ç–æ–∫\n",
        "combined_stream = itertools.chain(source1(), source2(), source3())\n",
        "\n",
        "print(\"–û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –ø–æ—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "for text in combined_stream:\n",
        "    print(f\"üìÑ {text}\")\n",
        "\n",
        "# –≠—Ç–æ—Ç –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –ø–æ—Ç–æ–∫ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –≤ encode_iterable:\n",
        "# token_stream = tokenizer.encode_iterable(combined_stream)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ –†–µ–∑—é–º–µ: –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å encode_iterable\n",
        "\n",
        "### ‚úÖ **–ò–¥–µ–∞–ª—å–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –¥–ª—è encode_iterable:**\n",
        "\n",
        "1. **üóÇÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤** - –ß–∏—Ç–∞—Ç—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã –ø–æ—Å—Ç—Ä–æ—á–Ω–æ –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ø–∞–º—è—Ç—å\n",
        "2. **üåä –ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞** - –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –º–µ—Ä–µ –∏—Ö –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è (—á–∞—Ç—ã, API, —Å–µ–Ω—Å–æ—Ä—ã)\n",
        "3. **üíæ –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏** - –ö–æ–≥–¥–∞ —É –≤–∞—Å –º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å\n",
        "4. **üîÑ –ü–∞–π–ø–ª–∞–π–Ω—ã –¥–∞–Ω–Ω—ã—Ö** - –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ø–æ—á–µ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏\n",
        "5. **‚ôæÔ∏è –ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –ø–æ—Ç–æ–∫–∏** - –†–∞–±–æ—Ç–∞ —Å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö\n",
        "6. **üéØ –õ–µ–Ω–∏–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è** - –ö–æ–≥–¥–∞ –Ω–µ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –Ω—É–∂–Ω—ã —Å—Ä–∞–∑—É\n",
        "\n",
        "### üîë **–ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:**\n",
        "\n",
        "- **Iterable** - –æ–±—ä–µ–∫—Ç, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ –∏—Ç–µ—Ä–∏—Ä–æ–≤–∞—Ç—å (`__iter__()`)\n",
        "- **Iterator** - –æ–±—ä–µ–∫—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç –∑–Ω–∞—á–µ–Ω–∏—è (`__next__()`, `__iter__()`)  \n",
        "- **Generator** - —Ñ—É–Ω–∫—Ü–∏—è —Å `yield`, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞—é—â–∞—è –∏—Ç–µ—Ä–∞—Ç–æ—Ä\n",
        "- **Lazy Evaluation** - –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é\n",
        "\n",
        "### üí° **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**\n",
        "\n",
        "```python\n",
        "# –í–º–µ—Å—Ç–æ:\n",
        "texts = [read_all_lines_from_file()]  # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å—ë –≤ –ø–∞–º—è—Ç—å\n",
        "tokens = tokenizer.encode(texts)\n",
        "\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ:\n",
        "text_stream = read_file_lines()       # –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å—Ç—Ä–æ–∫\n",
        "token_stream = tokenizer.encode_iterable(text_stream)  # –õ–µ–Ω–∏–≤–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "```\n",
        "\n",
        "### üöÄ **–†–µ–∑—É–ª—å—Ç–∞—Ç:**\n",
        "- **–ú–µ–Ω—å—à–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏** (–≤ —Å–æ—Ç–Ω–∏ —Ä–∞–∑!)\n",
        "- **–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–æ–≤ –ª—é–±–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞**\n",
        "- **–ì–∏–±–∫–∏–µ –ø–∞–π–ø–ª–∞–π–Ω—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö**\n",
        "- **–ü–æ—Ç–æ–∫–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
